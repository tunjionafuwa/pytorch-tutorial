{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab9a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b22c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('./training.1600000.processed.noemoticon.csv', engine='python', header=None, encoding='latin1')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7e4439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeea1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['sentiment_cat'] = tweets_df[0].astype('category')\n",
    "tweets_df['sentiment'] = tweets_df['sentiment_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9a4b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>sentiment_cat</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1321983</th>\n",
       "      <td>4</td>\n",
       "      <td>2014746526</td>\n",
       "      <td>Wed Jun 03 02:33:02 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>davidliew</td>\n",
       "      <td>Finally finished those many works today, now c...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131810</th>\n",
       "      <td>0</td>\n",
       "      <td>1835612983</td>\n",
       "      <td>Mon May 18 06:22:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kluper</td>\n",
       "      <td>@no634 aww the pre-finals paper jam happened t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394560</th>\n",
       "      <td>4</td>\n",
       "      <td>2053630459</td>\n",
       "      <td>Sat Jun 06 05:12:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>beanandgone</td>\n",
       "      <td>@hasbean coffee shop crawl?</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327881</th>\n",
       "      <td>0</td>\n",
       "      <td>2010150562</td>\n",
       "      <td>Tue Jun 02 16:53:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ERK</td>\n",
       "      <td>16GB SD still works after reformat. I just got...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493588</th>\n",
       "      <td>0</td>\n",
       "      <td>2184649495</td>\n",
       "      <td>Mon Jun 15 16:13:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>eireanneilis</td>\n",
       "      <td>needs a job that makes more than peanuts or ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477171</th>\n",
       "      <td>4</td>\n",
       "      <td>2066230315</td>\n",
       "      <td>Sun Jun 07 10:07:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TLM26</td>\n",
       "      <td>@runnersrambles thanks for running with my slo...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225027</th>\n",
       "      <td>4</td>\n",
       "      <td>1990629503</td>\n",
       "      <td>Mon Jun 01 05:21:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RAWRachael</td>\n",
       "      <td>@saltyshutter sandstone drive (by st Michaels ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85656</th>\n",
       "      <td>0</td>\n",
       "      <td>1753898765</td>\n",
       "      <td>Sun May 10 04:01:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lanied3ph</td>\n",
       "      <td>says work again tomorrow  http://plurk.com/p/s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378692</th>\n",
       "      <td>4</td>\n",
       "      <td>2052040131</td>\n",
       "      <td>Fri Jun 05 23:29:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>diveacademy</td>\n",
       "      <td>is diving at the Mogan Wrecks and Medio Elmud</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275155</th>\n",
       "      <td>4</td>\n",
       "      <td>2001054099</td>\n",
       "      <td>Mon Jun 01 23:42:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaokun</td>\n",
       "      <td>@tomlapille reading Learning Perl when I was i...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3             4  \\\n",
       "1321983  4  2014746526  Wed Jun 03 02:33:02 PDT 2009  NO_QUERY     davidliew   \n",
       "131810   0  1835612983  Mon May 18 06:22:55 PDT 2009  NO_QUERY        kluper   \n",
       "1394560  4  2053630459  Sat Jun 06 05:12:18 PDT 2009  NO_QUERY   beanandgone   \n",
       "327881   0  2010150562  Tue Jun 02 16:53:52 PDT 2009  NO_QUERY           ERK   \n",
       "493588   0  2184649495  Mon Jun 15 16:13:00 PDT 2009  NO_QUERY  eireanneilis   \n",
       "1477171  4  2066230315  Sun Jun 07 10:07:45 PDT 2009  NO_QUERY         TLM26   \n",
       "1225027  4  1990629503  Mon Jun 01 05:21:55 PDT 2009  NO_QUERY    RAWRachael   \n",
       "85656    0  1753898765  Sun May 10 04:01:22 PDT 2009  NO_QUERY     lanied3ph   \n",
       "1378692  4  2052040131  Fri Jun 05 23:29:13 PDT 2009  NO_QUERY   diveacademy   \n",
       "1275155  4  2001054099  Mon Jun 01 23:42:29 PDT 2009  NO_QUERY        kaokun   \n",
       "\n",
       "                                                         5 sentiment_cat  \\\n",
       "1321983  Finally finished those many works today, now c...             4   \n",
       "131810   @no634 aww the pre-finals paper jam happened t...             0   \n",
       "1394560                       @hasbean coffee shop crawl?              4   \n",
       "327881   16GB SD still works after reformat. I just got...             0   \n",
       "493588   needs a job that makes more than peanuts or ju...             0   \n",
       "1477171  @runnersrambles thanks for running with my slo...             4   \n",
       "1225027  @saltyshutter sandstone drive (by st Michaels ...             4   \n",
       "85656    says work again tomorrow  http://plurk.com/p/s...             0   \n",
       "1378692     is diving at the Mogan Wrecks and Medio Elmud              4   \n",
       "1275155  @tomlapille reading Learning Perl when I was i...             4   \n",
       "\n",
       "         sentiment  \n",
       "1321983          1  \n",
       "131810           0  \n",
       "1394560          1  \n",
       "327881           0  \n",
       "493588           0  \n",
       "1477171          1  \n",
       "1225027          1  \n",
       "85656            0  \n",
       "1378692          1  \n",
       "1275155          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc19b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('train-processed.csv', header=None, index=None)\n",
    "tweets_df.sample(10000).to_csv('train-processed-sample.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446aaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(tweet) -> list:\n",
    "    return [word.text.lower() for word in nlp(tweet)]\n",
    "\n",
    "\n",
    "def build_vocab(tweets, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tweet in tweets:\n",
    "        counter.update(tweet)\n",
    "\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    tweets = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return tweets, labels\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab=None) -> None:\n",
    "        self.tokens = [tokenize(tweet) for tweet in tweets]\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab or build_vocab(self.tokens)\n",
    "        self.data = [self.token_to_num(tokens) for tokens in self.tokens]\n",
    "    \n",
    "    def token_to_num(self, tokens):\n",
    "        return [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f207c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "sample_tweets_df = pd.read_csv('./train-processed-sample.csv', header=None)\n",
    "\n",
    "tweets = sample_tweets_df.loc[:, 5]\n",
    "labels = sample_tweets_df.loc[:, 7]\n",
    "\n",
    "tweets_train, tweets_temp, labels_train, labels_temp = train_test_split(\n",
    "    tweets, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "tweets_val, tweets_test, labels_val, labels_test = train_test_split(\n",
    "    tweets_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    ")\n",
    "\n",
    "train_data = TweetDataset(tweets=tweets_train, labels=labels_train)\n",
    "val_data = TweetDataset(tweets=tweets_val, labels=labels_val)\n",
    "test_data = TweetDataset(tweets=tweets_test, labels=labels_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, shuffle=True, batch_size=64, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=64, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a1259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a55862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b58ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d44510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eef393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9f2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e05bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [tok.text.lower() for tok in nlp(text)]\n",
    "\n",
    "\n",
    "def build_vocab(token_lists, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(word)\n",
    "    \n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab=None):\n",
    "        self.tokens = [tokenize(tweet) for tweet in tweets]\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab or build_vocab(self.tokens)\n",
    "        self.data = [self.numericalize(tokens) for tokens in self.tokens]\n",
    "\n",
    "    def numericalize(self, tokens):\n",
    "        return [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    tweets = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "sample_tweets_df = pd.read_csv('./train-processed-sample.csv', header=None)\n",
    "\n",
    "all_tweets = sample_tweets_df.loc[:, 5]\n",
    "all_labels = sample_tweets_df.loc[:, 7]\n",
    "\n",
    "tweets_train, tweets_temp, labels_train, labels_temp = train_test_split(\n",
    "    all_tweets, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "tweets_val, tweets_test, labels_val, labels_test = train_test_split(\n",
    "    tweets_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    ")\n",
    "\n",
    "train_data = TweetDataset(tweets=tweets_train, labels=labels_train)\n",
    "val_data = TweetDataset(tweets=tweets_val, labels=labels_val)\n",
    "test_data = TweetDataset(tweets=tweets_test, labels=labels_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f220c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab30cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6a680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39664eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mat_embed = nn.Embedding(5, 2)\n",
    "sentences = torch.arange(5)\n",
    "cat_mat_embed.forward(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c530190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
