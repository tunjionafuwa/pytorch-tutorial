{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab9a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b22c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('./training.1600000.processed.noemoticon.csv', engine='python', header=None, encoding='latin1')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7e4439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeea1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['sentiment_cat'] = tweets_df[0].astype('category')\n",
    "tweets_df['sentiment'] = tweets_df['sentiment_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9a4b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>sentiment_cat</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>832206</th>\n",
       "      <td>4</td>\n",
       "      <td>1557555594</td>\n",
       "      <td>Sun Apr 19 04:48:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JamesHancox</td>\n",
       "      <td>@Courageous_one Awww... she just wants to be c...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585102</th>\n",
       "      <td>0</td>\n",
       "      <td>2215511752</td>\n",
       "      <td>Wed Jun 17 18:14:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>valsan71</td>\n",
       "      <td>@dannygokey Iam so sad I missed your chat sees...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323916</th>\n",
       "      <td>0</td>\n",
       "      <td>2005830414</td>\n",
       "      <td>Tue Jun 02 10:14:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChelseySyrnyk</td>\n",
       "      <td>No sleep last night. And my mind is still a cl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141358</th>\n",
       "      <td>4</td>\n",
       "      <td>1977169998</td>\n",
       "      <td>Sat May 30 20:16:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HaasDesigns</td>\n",
       "      <td>HaasDesignsis finally home for the night. Wher...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838953</th>\n",
       "      <td>4</td>\n",
       "      <td>1559257355</td>\n",
       "      <td>Sun Apr 19 10:40:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LindsayWhite</td>\n",
       "      <td>@paulaabdul have a great sunday! Have fun danc...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411945</th>\n",
       "      <td>4</td>\n",
       "      <td>2056571878</td>\n",
       "      <td>Sat Jun 06 11:23:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xosaraa</td>\n",
       "      <td>@JessChristine13 I know I have a problem</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29278</th>\n",
       "      <td>0</td>\n",
       "      <td>1562360062</td>\n",
       "      <td>Sun Apr 19 19:47:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>brucemjackson</td>\n",
       "      <td>@BeckyBuckwild wonder if u told him about the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550088</th>\n",
       "      <td>4</td>\n",
       "      <td>2183696539</td>\n",
       "      <td>Mon Jun 15 14:51:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vortexsquid</td>\n",
       "      <td>Yeah i ended up ordering the dvd AND watching ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005565</th>\n",
       "      <td>4</td>\n",
       "      <td>1880524238</td>\n",
       "      <td>Fri May 22 01:33:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>freeek0804</td>\n",
       "      <td>@AshleyLTMSYF http://twitpic.com/5o7al - You l...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675519</th>\n",
       "      <td>0</td>\n",
       "      <td>2248119186</td>\n",
       "      <td>Fri Jun 19 20:07:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tifanguyen</td>\n",
       "      <td>#dontyouhate exams?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3              4  \\\n",
       "832206   4  1557555594  Sun Apr 19 04:48:51 PDT 2009  NO_QUERY    JamesHancox   \n",
       "585102   0  2215511752  Wed Jun 17 18:14:30 PDT 2009  NO_QUERY       valsan71   \n",
       "323916   0  2005830414  Tue Jun 02 10:14:57 PDT 2009  NO_QUERY  ChelseySyrnyk   \n",
       "1141358  4  1977169998  Sat May 30 20:16:46 PDT 2009  NO_QUERY    HaasDesigns   \n",
       "838953   4  1559257355  Sun Apr 19 10:40:27 PDT 2009  NO_QUERY   LindsayWhite   \n",
       "1411945  4  2056571878  Sat Jun 06 11:23:26 PDT 2009  NO_QUERY        xosaraa   \n",
       "29278    0  1562360062  Sun Apr 19 19:47:55 PDT 2009  NO_QUERY  brucemjackson   \n",
       "1550088  4  2183696539  Mon Jun 15 14:51:40 PDT 2009  NO_QUERY    vortexsquid   \n",
       "1005565  4  1880524238  Fri May 22 01:33:04 PDT 2009  NO_QUERY     freeek0804   \n",
       "675519   0  2248119186  Fri Jun 19 20:07:50 PDT 2009  NO_QUERY     tifanguyen   \n",
       "\n",
       "                                                         5 sentiment_cat  \\\n",
       "832206   @Courageous_one Awww... she just wants to be c...             4   \n",
       "585102   @dannygokey Iam so sad I missed your chat sees...             0   \n",
       "323916   No sleep last night. And my mind is still a cl...             0   \n",
       "1141358  HaasDesignsis finally home for the night. Wher...             4   \n",
       "838953   @paulaabdul have a great sunday! Have fun danc...             4   \n",
       "1411945          @JessChristine13 I know I have a problem              4   \n",
       "29278    @BeckyBuckwild wonder if u told him about the ...             0   \n",
       "1550088  Yeah i ended up ordering the dvd AND watching ...             4   \n",
       "1005565  @AshleyLTMSYF http://twitpic.com/5o7al - You l...             4   \n",
       "675519                                #dontyouhate exams?              0   \n",
       "\n",
       "         sentiment  \n",
       "832206           1  \n",
       "585102           0  \n",
       "323916           0  \n",
       "1141358          1  \n",
       "838953           1  \n",
       "1411945          1  \n",
       "29278            0  \n",
       "1550088          1  \n",
       "1005565          1  \n",
       "675519           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc19b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('train-processed.csv', header=None, index=None)\n",
    "tweets_df.sample(10000).to_csv('train-processed-sample.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446aaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(tweet) -> list:\n",
    "    return [word.text.lower() for word in nlp(tweet)]\n",
    "\n",
    "\n",
    "def build_vocab(tweets, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tweet in tweets:\n",
    "        counter.update(tweet)\n",
    "\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tweets, labels = zip(*batch)\n",
    "    tweets = pad_sequence(tweets, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return tweets, labels\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, labels, vocab=None) -> None:\n",
    "        self.tokens = [tokenize(tweet) for tweet in tweets]\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab or build_vocab(self.tokens)\n",
    "        self.data = [self.token_to_num(tokens) for tokens in self.tokens]\n",
    "    \n",
    "    def token_to_num(self, tokens):\n",
    "        return [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]), torch.tensor(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f207c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "sample_tweets_df = pd.read_csv('./train-processed-sample.csv', header=None)\n",
    "\n",
    "tweets = sample_tweets_df.loc[:, 5].values\n",
    "labels = sample_tweets_df.loc[:, 7].values\n",
    "\n",
    "tweets_train, tweets_temp, labels_train, labels_temp = train_test_split(\n",
    "    tweets, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "tweets_val, tweets_test, labels_val, labels_test = train_test_split(\n",
    "    tweets_temp, labels_temp, test_size=0.5, random_state=42, stratify=labels_temp\n",
    ")\n",
    "\n",
    "train_data = TweetDataset(tweets=tweets_train, labels=labels_train)\n",
    "val_data = TweetDataset(tweets=tweets_val, labels=labels_val)\n",
    "test_data = TweetDataset(tweets=tweets_test, labels=labels_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, shuffle=True, batch_size=64, num_workers=num_workers, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=64, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5b4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.predictor = nn.Linear(in_features=hidden_size, out_features=2)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        output, (hidden, _) = self.encoder(self.embedding(seq))\n",
    "        preds = self.predictor(hidden.squeeze(0))\n",
    "        return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9f6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3213c7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(20002, 100)\n",
       "  (encoder): LSTM(100, 300, batch_first=True)\n",
       "  (predictor): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lstm = SentimentLSTM(100, 300, 20002)\n",
    "sentiment_lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff7a1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=sentiment_lstm.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51d44510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/xm_8gc6x0hncjzlyy0hscm7h0000gn/T/ipykernel_9740/1395221962.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.70, Validation Loss: 0.69, Accuracy: 0.50\n",
      "Epoch: 2, Train Loss: 0.69, Validation Loss: 0.69, Accuracy: 0.50\n",
      "Epoch: 3, Train Loss: 0.68, Validation Loss: 0.72, Accuracy: 0.50\n",
      "Epoch: 4, Train Loss: 0.60, Validation Loss: 0.81, Accuracy: 0.48\n",
      "Epoch: 5, Train Loss: 0.49, Validation Loss: 0.86, Accuracy: 0.51\n",
      "Epoch: 6, Train Loss: 0.39, Validation Loss: 0.99, Accuracy: 0.51\n",
      "Epoch: 7, Train Loss: 0.29, Validation Loss: 1.31, Accuracy: 0.51\n",
      "Epoch: 8, Train Loss: 0.19, Validation Loss: 1.39, Accuracy: 0.52\n",
      "Epoch: 9, Train Loss: 0.12, Validation Loss: 1.78, Accuracy: 0.52\n",
      "Epoch: 10, Train Loss: 0.07, Validation Loss: 2.19, Accuracy: 0.52\n",
      "Epoch: 11, Train Loss: 0.06, Validation Loss: 2.23, Accuracy: 0.52\n",
      "Epoch: 12, Train Loss: 0.04, Validation Loss: 2.11, Accuracy: 0.50\n",
      "Epoch: 13, Train Loss: 0.04, Validation Loss: 2.69, Accuracy: 0.52\n",
      "Epoch: 14, Train Loss: 0.03, Validation Loss: 2.77, Accuracy: 0.52\n",
      "Epoch: 15, Train Loss: 0.03, Validation Loss: 2.59, Accuracy: 0.52\n",
      "Epoch: 16, Train Loss: 0.02, Validation Loss: 3.08, Accuracy: 0.54\n",
      "Epoch: 17, Train Loss: 0.03, Validation Loss: 2.80, Accuracy: 0.53\n",
      "Epoch: 18, Train Loss: 0.01, Validation Loss: 3.53, Accuracy: 0.52\n",
      "Epoch: 19, Train Loss: 0.01, Validation Loss: 3.52, Accuracy: 0.51\n",
      "Epoch: 20, Train Loss: 0.01, Validation Loss: 3.51, Accuracy: 0.52\n",
      "Epoch: 21, Train Loss: 0.01, Validation Loss: 3.47, Accuracy: 0.52\n",
      "Epoch: 22, Train Loss: 0.00, Validation Loss: 3.67, Accuracy: 0.51\n",
      "Epoch: 23, Train Loss: 0.01, Validation Loss: 3.47, Accuracy: 0.51\n",
      "Epoch: 24, Train Loss: 0.01, Validation Loss: 3.68, Accuracy: 0.52\n",
      "Epoch: 25, Train Loss: 0.00, Validation Loss: 3.83, Accuracy: 0.52\n",
      "Epoch: 26, Train Loss: 0.00, Validation Loss: 4.07, Accuracy: 0.52\n",
      "Epoch: 27, Train Loss: 0.00, Validation Loss: 4.03, Accuracy: 0.51\n",
      "Epoch: 28, Train Loss: 0.00, Validation Loss: 4.18, Accuracy: 0.52\n",
      "Epoch: 29, Train Loss: 0.00, Validation Loss: 4.10, Accuracy: 0.52\n",
      "Epoch: 30, Train Loss: 0.00, Validation Loss: 4.48, Accuracy: 0.52\n",
      "Epoch: 31, Train Loss: 0.00, Validation Loss: 4.48, Accuracy: 0.52\n",
      "Epoch: 32, Train Loss: 0.00, Validation Loss: 4.55, Accuracy: 0.51\n",
      "Epoch: 33, Train Loss: 0.00, Validation Loss: 4.53, Accuracy: 0.52\n",
      "Epoch: 34, Train Loss: 0.00, Validation Loss: 4.49, Accuracy: 0.51\n",
      "Epoch: 35, Train Loss: 0.00, Validation Loss: 4.71, Accuracy: 0.51\n",
      "Epoch: 36, Train Loss: 0.00, Validation Loss: 4.76, Accuracy: 0.52\n",
      "Epoch: 37, Train Loss: 0.00, Validation Loss: 4.77, Accuracy: 0.51\n",
      "Epoch: 38, Train Loss: 0.00, Validation Loss: 4.81, Accuracy: 0.52\n",
      "Epoch: 39, Train Loss: 0.00, Validation Loss: 4.94, Accuracy: 0.51\n",
      "Epoch: 40, Train Loss: 0.00, Validation Loss: 5.01, Accuracy: 0.51\n",
      "Epoch: 41, Train Loss: 0.00, Validation Loss: 4.94, Accuracy: 0.52\n",
      "Epoch: 42, Train Loss: 0.00, Validation Loss: 5.05, Accuracy: 0.52\n",
      "Epoch: 43, Train Loss: 0.00, Validation Loss: 5.16, Accuracy: 0.51\n",
      "Epoch: 44, Train Loss: 0.00, Validation Loss: 5.18, Accuracy: 0.51\n",
      "Epoch: 45, Train Loss: 0.00, Validation Loss: 5.12, Accuracy: 0.52\n",
      "Epoch: 46, Train Loss: 0.00, Validation Loss: 5.11, Accuracy: 0.52\n",
      "Epoch: 47, Train Loss: 0.00, Validation Loss: 5.10, Accuracy: 0.52\n",
      "Epoch: 48, Train Loss: 0.00, Validation Loss: 5.39, Accuracy: 0.52\n",
      "Epoch: 49, Train Loss: 0.00, Validation Loss: 5.19, Accuracy: 0.52\n",
      "Epoch: 50, Train Loss: 0.00, Validation Loss: 5.37, Accuracy: 0.52\n",
      "Epoch: 51, Train Loss: 0.00, Validation Loss: 5.31, Accuracy: 0.52\n",
      "Epoch: 52, Train Loss: 0.00, Validation Loss: 5.33, Accuracy: 0.51\n",
      "Epoch: 53, Train Loss: 0.00, Validation Loss: 5.44, Accuracy: 0.52\n",
      "Epoch: 54, Train Loss: 0.00, Validation Loss: 5.48, Accuracy: 0.52\n",
      "Epoch: 55, Train Loss: 0.00, Validation Loss: 5.56, Accuracy: 0.52\n",
      "Epoch: 56, Train Loss: 0.00, Validation Loss: 5.63, Accuracy: 0.52\n",
      "Epoch: 57, Train Loss: 0.00, Validation Loss: 5.73, Accuracy: 0.52\n",
      "Epoch: 58, Train Loss: 0.00, Validation Loss: 5.79, Accuracy: 0.52\n",
      "Epoch: 59, Train Loss: 0.00, Validation Loss: 5.66, Accuracy: 0.52\n",
      "Epoch: 60, Train Loss: 0.00, Validation Loss: 5.61, Accuracy: 0.52\n",
      "Epoch: 61, Train Loss: 0.00, Validation Loss: 5.81, Accuracy: 0.52\n",
      "Epoch: 62, Train Loss: 0.00, Validation Loss: 5.79, Accuracy: 0.52\n",
      "Epoch: 63, Train Loss: 0.00, Validation Loss: 5.78, Accuracy: 0.52\n",
      "Epoch: 64, Train Loss: 0.00, Validation Loss: 5.82, Accuracy: 0.52\n",
      "Epoch: 65, Train Loss: 0.00, Validation Loss: 5.86, Accuracy: 0.52\n",
      "Epoch: 66, Train Loss: 0.00, Validation Loss: 5.87, Accuracy: 0.52\n",
      "Epoch: 67, Train Loss: 0.00, Validation Loss: 5.84, Accuracy: 0.52\n",
      "Epoch: 68, Train Loss: 0.00, Validation Loss: 5.84, Accuracy: 0.52\n",
      "Epoch: 69, Train Loss: 0.00, Validation Loss: 5.92, Accuracy: 0.52\n",
      "Epoch: 70, Train Loss: 0.00, Validation Loss: 5.95, Accuracy: 0.52\n",
      "Epoch: 71, Train Loss: 0.00, Validation Loss: 6.07, Accuracy: 0.52\n",
      "Epoch: 72, Train Loss: 0.00, Validation Loss: 6.05, Accuracy: 0.52\n",
      "Epoch: 73, Train Loss: 0.00, Validation Loss: 6.15, Accuracy: 0.52\n",
      "Epoch: 74, Train Loss: 0.00, Validation Loss: 6.02, Accuracy: 0.52\n",
      "Epoch: 75, Train Loss: 0.00, Validation Loss: 5.99, Accuracy: 0.52\n",
      "Epoch: 76, Train Loss: 0.01, Validation Loss: 2.13, Accuracy: 0.51\n",
      "Epoch: 77, Train Loss: 0.09, Validation Loss: 2.18, Accuracy: 0.51\n",
      "Epoch: 78, Train Loss: 0.02, Validation Loss: 2.56, Accuracy: 0.52\n",
      "Epoch: 79, Train Loss: 0.01, Validation Loss: 3.28, Accuracy: 0.53\n",
      "Epoch: 80, Train Loss: 0.00, Validation Loss: 3.96, Accuracy: 0.53\n",
      "Epoch: 81, Train Loss: 0.00, Validation Loss: 4.14, Accuracy: 0.53\n",
      "Epoch: 82, Train Loss: 0.00, Validation Loss: 4.27, Accuracy: 0.53\n",
      "Epoch: 83, Train Loss: 0.00, Validation Loss: 4.28, Accuracy: 0.53\n",
      "Epoch: 84, Train Loss: 0.00, Validation Loss: 4.41, Accuracy: 0.52\n",
      "Epoch: 85, Train Loss: 0.00, Validation Loss: 4.41, Accuracy: 0.52\n",
      "Epoch: 86, Train Loss: 0.00, Validation Loss: 4.47, Accuracy: 0.52\n",
      "Epoch: 87, Train Loss: 0.00, Validation Loss: 4.65, Accuracy: 0.52\n",
      "Epoch: 88, Train Loss: 0.00, Validation Loss: 4.62, Accuracy: 0.52\n",
      "Epoch: 89, Train Loss: 0.00, Validation Loss: 4.65, Accuracy: 0.52\n",
      "Epoch: 90, Train Loss: 0.00, Validation Loss: 4.71, Accuracy: 0.53\n",
      "Epoch: 91, Train Loss: 0.00, Validation Loss: 4.73, Accuracy: 0.52\n",
      "Epoch: 92, Train Loss: 0.00, Validation Loss: 4.80, Accuracy: 0.53\n",
      "Epoch: 93, Train Loss: 0.00, Validation Loss: 4.88, Accuracy: 0.53\n",
      "Epoch: 94, Train Loss: 0.00, Validation Loss: 4.92, Accuracy: 0.52\n",
      "Epoch: 95, Train Loss: 0.00, Validation Loss: 4.97, Accuracy: 0.52\n",
      "Epoch: 96, Train Loss: 0.00, Validation Loss: 5.01, Accuracy: 0.52\n",
      "Epoch: 97, Train Loss: 0.00, Validation Loss: 5.06, Accuracy: 0.52\n",
      "Epoch: 98, Train Loss: 0.00, Validation Loss: 5.04, Accuracy: 0.52\n",
      "Epoch: 99, Train Loss: 0.00, Validation Loss: 5.13, Accuracy: 0.52\n",
      "Epoch: 100, Train Loss: 0.00, Validation Loss: 5.14, Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "def train(model=sentiment_lstm, train_loader=train_loader, val_loader=val_loader, loss_fn=criterion, optimizer=optimizer, device=device, epochs=100):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()\n",
    "        for inputs, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        num_correct = 0.0\n",
    "        num_examples = 0.0\n",
    "        model.eval()\n",
    "        for inputs, target in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss:.2f}, Validation Loss: {val_loss:.2f}, Accuracy: {(num_correct/num_examples):.2f}\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11eef393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e9f2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5173333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=sentiment_lstm, dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e05bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c530190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
